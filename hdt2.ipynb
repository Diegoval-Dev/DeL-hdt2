{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3939167c",
   "metadata": {},
   "source": [
    "# CC3092 — Deep Learning – Hoja de Trabajo 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75409da3",
   "metadata": {},
   "source": [
    "Integrantes: \n",
    "Diego Valenzuela - 22309\n",
    "Gerson Ramirez - 22281"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79809274",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch version: 2.8.0+cu126\n",
      "CUDA disponible: True\n",
      "GPU: NVIDIA GeForce GTX 1660 Ti with Max-Q Design\n",
      "Tensor en dispositivo: cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"torch version:\", torch.__version__)\n",
    "print(\"CUDA disponible:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
    "    x = torch.rand((2, 2), device=\"cuda\")\n",
    "    print(\"Tensor en dispositivo:\", x.device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0c4ddec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time, math, random\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def set_seed(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.use_deterministic_algorithms(False)  \n",
    "set_seed(42)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9487e5",
   "metadata": {},
   "source": [
    "## Task 1 — Carga de Iris + split (train/val) + escalado"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b27a70d",
   "metadata": {},
   "source": [
    "Se usa Iris para clasificación multiclase (3 clases). Se separa 75/25 (train/val) con estratificación para mantener proporciones de clases. Se estandarizan atributos (media 0, var 1), lo que acelera y estabiliza el entrenamiento del MLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e67f236",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(112, 38)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris = load_iris()\n",
    "X = iris.data.astype(np.float32)\n",
    "y = iris.target.astype(np.int64)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train).astype(np.float32)\n",
    "X_val   = scaler.transform(X_val).astype(np.float32)\n",
    "\n",
    "X_train_t = torch.from_numpy(X_train)\n",
    "y_train_t = torch.from_numpy(y_train)\n",
    "X_val_t   = torch.from_numpy(X_val)\n",
    "y_val_t   = torch.from_numpy(y_val)\n",
    "\n",
    "train_ds = TensorDataset(X_train_t, y_train_t)\n",
    "val_ds   = TensorDataset(X_val_t,   y_val_t)\n",
    "\n",
    "len(train_ds), len(val_ds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e34a523c",
   "metadata": {},
   "source": [
    "## Task 2 — MLP simple y parametrizable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "735f46c6",
   "metadata": {},
   "source": [
    "Arquitectura feedforward con capas ocultas configurables, activación seleccionable y Dropout (para Task 4). La capa final entrega logits (sin softmax); la función de pérdida se encarga de lo demás."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db129c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_dim: int = 4, hidden: List[int] = [32, 16],\n",
    "                 out_dim: int = 3, activation: str = \"relu\", dropout_p: float = 0.0):\n",
    "        super().__init__()\n",
    "        acts = {\n",
    "            \"relu\": nn.ReLU(),\n",
    "            \"tanh\": nn.Tanh(),\n",
    "            \"gelu\": nn.GELU(),\n",
    "            \"leakyrelu\": nn.LeakyReLU(0.1),\n",
    "        }\n",
    "        self.act = acts.get(activation.lower(), nn.ReLU())\n",
    "        layers = []\n",
    "        prev = in_dim\n",
    "        for h in hidden:\n",
    "            layers += [nn.Linear(prev, h), self.act]\n",
    "            if dropout_p and dropout_p > 0.0:\n",
    "                layers += [nn.Dropout(dropout_p)]\n",
    "            prev = h\n",
    "        layers += [nn.Linear(prev, out_dim)]\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd3032d1",
   "metadata": {},
   "source": [
    "## 5) Task 3 — Funciones de pérdida (CE, NLL, MSE) parametrizadas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23343047",
   "metadata": {},
   "source": [
    "Usaremos CrossEntropyLoss, NLLLoss (con log_softmax) y MSE (con one-hot + softmax). Esto cumple el requisito de ≥3 pérdidas y permite comparar convergencia y rendimiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a77778e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class LossAdapter:\n",
    "    name: str\n",
    "    criterion: nn.Module\n",
    "    def __call__(self, logits: torch.Tensor, y: torch.Tensor) -> torch.Tensor:\n",
    "        if self.name == \"cross_entropy\":\n",
    "            return self.criterion(logits, y)\n",
    "        elif self.name == \"nll\":\n",
    "            return self.criterion(F.log_softmax(logits, dim=1), y)\n",
    "        elif self.name == \"mse\":\n",
    "            probs = F.softmax(logits, dim=1)\n",
    "            one_hot = F.one_hot(y, num_classes=logits.shape[1]).float()\n",
    "            return self.criterion(probs, one_hot)\n",
    "        else:\n",
    "            raise ValueError(f\"Pérdida desconocida: {self.name}\")\n",
    "\n",
    "def make_loss(name: str) -> LossAdapter:\n",
    "    name = name.lower()\n",
    "    if name in (\"crossentropy\", \"ce\", \"cross_entropy\"):\n",
    "        return LossAdapter(\"cross_entropy\", nn.CrossEntropyLoss())\n",
    "    if name in (\"nll\", \"nllloss\"):\n",
    "        return LossAdapter(\"nll\", nn.NLLLoss())\n",
    "    if name in (\"mse\", \"mseloss\"):\n",
    "        return LossAdapter(\"mse\", nn.MSELoss())\n",
    "    raise ValueError(f\"Pérdida no soportada: {name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5834978c",
   "metadata": {},
   "source": [
    "## Task 4 — Regularización (L1, L2, Dropout)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a9295c",
   "metadata": {},
   "source": [
    "* L2: se usa como weight_decay del optimizador.\n",
    "* L1: se suma manualmente a la pérdida: λ₁ * Σ|w|.\n",
    "* Dropout: parametrizado en el modelo (ya incluido)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fbd2f8f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def l1_penalty(model: nn.Module) -> torch.Tensor:\n",
    "    total = torch.tensor(0., device=device)\n",
    "    for p in model.parameters():\n",
    "        if p.requires_grad:\n",
    "            total = total + p.abs().sum()\n",
    "    return total\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "254ae999",
   "metadata": {},
   "source": [
    "## Task 5 — “Algoritmos de optimización”"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72ebf7a",
   "metadata": {},
   "source": [
    "* Batch GD: actualiza con todo el conjunto de entrenamiento (batch único).\n",
    "* Mini-Batch GD: actualiza por lotes pequeños (p. ej. 16).\n",
    "* SGD: actualiza por cada muestra (batch_size=1).\n",
    "\n",
    "> Usaremos el mismo optimizador (torch.optim.SGD) pero cambiaremos el tamaño del batch para reflejar cada técnica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "73ab957a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_loader(dataset, mode: str, batch_size: int = 16, shuffle: bool = True):\n",
    "    mode = mode.lower()\n",
    "    if mode == \"batch_gd\":\n",
    "        bs = len(dataset)\n",
    "    elif mode == \"sgd\":\n",
    "        bs = 1\n",
    "    elif mode == \"mini-batch\" or mode == \"mini_batch\":\n",
    "        bs = batch_size\n",
    "    else:\n",
    "        raise ValueError(\"mode debe ser 'batch_gd', 'mini-batch' o 'sgd'\")\n",
    "    return DataLoader(dataset, batch_size=bs, shuffle=shuffle)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e754d5",
   "metadata": {},
   "source": [
    "# Parte 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "314041e3",
   "metadata": {},
   "source": [
    "1. **¿Cuál es la principal innovación de la arquitectura Transformer?**\n",
    "\n",
    "  La gran innovación del Transformer es eliminar por completo las recurrencias y convoluciones. En su lugar, se basa únicamente en mecanismos de atención, especialmente *self-attention*, para modelar dependencias entre tokens. Esto permite mayor paralelización en el entrenamiento y mejor manejo de dependencias largas.\n",
    "\n",
    "2. **¿Cómo funciona el mecanismo de atención del scaled dot-product?**\n",
    "\n",
    "  El scaled dot-product attention toma consultas (Q), claves (K) y valores (V). Calcula los productos punto entre las consultas y todas las claves, los escala por $1/\\sqrt{d_k}$ para evitar gradientes muy pequeños, y aplica softmax para obtener pesos de atención. Luego usa esos pesos para combinar linealmente los valores:\n",
    "\n",
    "  $$\n",
    "  \\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\n",
    "  $$\n",
    "\n",
    "  Este escalado es lo que diferencia al mecanismo y lo hace más estable.\n",
    "\n",
    "3. **¿Por qué se utiliza la atención de múltiples cabezales en Transformer?**\n",
    "\n",
    "  La multi-head attention proyecta las Q, K y V en distintos subespacios, aplica atención en paralelo y concatena los resultados. Esto permite que el modelo aprenda a atender a diferentes aspectos de la información en paralelo, como relaciones sintácticas y semánticas distintas. Con un único “head”, la información se promediaría y se perderían matices\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
