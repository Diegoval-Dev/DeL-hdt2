{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3939167c",
   "metadata": {},
   "source": [
    "# CC3092 — Deep Learning – Hoja de Trabajo 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75409da3",
   "metadata": {},
   "source": [
    "Integrantes: \n",
    "Diego Valenzuela - 22309\n",
    "Gerson Ramirez - 22281"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13914fa7",
   "metadata": {},
   "source": [
    "**Compatibilidad:** El archivo de requirements.txt fue establecido para la instalación de dependencias específicas para trabajar con el proyecto utilizando Apple Silicon Chips."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79809274",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch version: 2.8.0\n",
      "MPS disponible: True\n",
      "MPS creado: True\n",
      "GPU: Apple Silicon Metal Performance Shaders\n",
      "Tensor en dispositivo: mps:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"torch version:\", torch.__version__)\n",
    "print(\"MPS disponible:\", torch.backends.mps.is_available())\n",
    "print(\"MPS creado:\", torch.backends.mps.is_built())\n",
    "if torch.backends.mps.is_available():\n",
    "    print(\"GPU: Apple Silicon Metal Performance Shaders\")\n",
    "    x = torch.rand((2, 2), device=\"mps\")\n",
    "    print(\"Tensor en dispositivo:\", x.device)\n",
    "elif torch.cuda.is_available():\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
    "    x = torch.rand((2, 2), device=\"cuda\")\n",
    "    print(\"Tensor en dispositivo:\", x.device)\n",
    "else:\n",
    "    print(\"Usando CPU\")\n",
    "    x = torch.rand((2, 2), device=\"cpu\")\n",
    "    print(\"Tensor en dispositivo:\", x.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0c4ddec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='mps')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time, math, random\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def set_seed(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.use_deterministic_algorithms(False)  \n",
    "set_seed(42)\n",
    "\n",
    "# Apple Silicon optimized device selection\n",
    "def get_device():\n",
    "    if torch.backends.mps.is_available() and torch.backends.mps.is_built():\n",
    "        return torch.device(\"mps\")\n",
    "    elif torch.cuda.is_available():\n",
    "        return torch.device(\"cuda\")\n",
    "    else:\n",
    "        return torch.device(\"cpu\")\n",
    "\n",
    "device = get_device()\n",
    "print(f\"Using device: {device}\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9487e5",
   "metadata": {},
   "source": [
    "## Task 1 — Carga de Iris + split (train/val) + escalado"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b27a70d",
   "metadata": {},
   "source": [
    "Se usa Iris para clasificación multiclase (3 clases). Se separa 75/25 (train/val) con estratificación para mantener proporciones de clases. Se estandarizan atributos (media 0, var 1), lo que acelera y estabiliza el entrenamiento del MLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e67f236",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(112, 38)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris = load_iris()\n",
    "X = iris.data.astype(np.float32)\n",
    "y = iris.target.astype(np.int64)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train).astype(np.float32)\n",
    "X_val   = scaler.transform(X_val).astype(np.float32)\n",
    "\n",
    "# Move tensors to the appropriate device (MPS for Apple Silicon)\n",
    "X_train_t = torch.from_numpy(X_train).to(device)\n",
    "y_train_t = torch.from_numpy(y_train).to(device)\n",
    "X_val_t   = torch.from_numpy(X_val).to(device)\n",
    "y_val_t   = torch.from_numpy(y_val).to(device)\n",
    "\n",
    "train_ds = TensorDataset(X_train_t, y_train_t)\n",
    "val_ds   = TensorDataset(X_val_t,   y_val_t)\n",
    "\n",
    "len(train_ds), len(val_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e34a523c",
   "metadata": {},
   "source": [
    "## Task 2 — MLP simple y parametrizable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "735f46c6",
   "metadata": {},
   "source": [
    "Arquitectura feedforward con capas ocultas configurables, activación seleccionable y Dropout (para Task 4). La capa final entrega logits (sin softmax); la función de pérdida se encarga de lo demás."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db129c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_dim: int = 4, hidden: List[int] = [32, 16],\n",
    "                 out_dim: int = 3, activation: str = \"relu\", dropout_p: float = 0.0):\n",
    "        super().__init__()\n",
    "        acts = {\n",
    "            \"relu\": nn.ReLU(),\n",
    "            \"tanh\": nn.Tanh(),\n",
    "            \"gelu\": nn.GELU(),\n",
    "            \"leakyrelu\": nn.LeakyReLU(0.1),\n",
    "        }\n",
    "        self.act = acts.get(activation.lower(), nn.ReLU())\n",
    "        layers = []\n",
    "        prev = in_dim\n",
    "        for h in hidden:\n",
    "            layers += [nn.Linear(prev, h), self.act]\n",
    "            if dropout_p and dropout_p > 0.0:\n",
    "                layers += [nn.Dropout(dropout_p)]\n",
    "            prev = h\n",
    "        layers += [nn.Linear(prev, out_dim)]\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd3032d1",
   "metadata": {},
   "source": [
    "## 5) Task 3 — Funciones de pérdida (CE, NLL, MSE) parametrizadas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23343047",
   "metadata": {},
   "source": [
    "Usaremos CrossEntropyLoss, NLLLoss (con log_softmax) y MSE (con one-hot + softmax). Esto cumple el requisito de ≥3 pérdidas y permite comparar convergencia y rendimiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a77778e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class LossAdapter:\n",
    "    name: str\n",
    "    criterion: nn.Module\n",
    "    def __call__(self, logits: torch.Tensor, y: torch.Tensor) -> torch.Tensor:\n",
    "        if self.name == \"cross_entropy\":\n",
    "            return self.criterion(logits, y)\n",
    "        elif self.name == \"nll\":\n",
    "            return self.criterion(F.log_softmax(logits, dim=1), y)\n",
    "        elif self.name == \"mse\":\n",
    "            probs = F.softmax(logits, dim=1)\n",
    "            one_hot = F.one_hot(y, num_classes=logits.shape[1]).float()\n",
    "            return self.criterion(probs, one_hot)\n",
    "        else:\n",
    "            raise ValueError(f\"Pérdida desconocida: {self.name}\")\n",
    "\n",
    "def make_loss(name: str) -> LossAdapter:\n",
    "    name = name.lower()\n",
    "    if name in (\"crossentropy\", \"ce\", \"cross_entropy\"):\n",
    "        return LossAdapter(\"cross_entropy\", nn.CrossEntropyLoss())\n",
    "    if name in (\"nll\", \"nllloss\"):\n",
    "        return LossAdapter(\"nll\", nn.NLLLoss())\n",
    "    if name in (\"mse\", \"mseloss\"):\n",
    "        return LossAdapter(\"mse\", nn.MSELoss())\n",
    "    raise ValueError(f\"Pérdida no soportada: {name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5834978c",
   "metadata": {},
   "source": [
    "## Task 4 — Regularización (L1, L2, Dropout)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a9295c",
   "metadata": {},
   "source": [
    "* L2: se usa como weight_decay del optimizador.\n",
    "* L1: se suma manualmente a la pérdida: λ₁ * Σ|w|.\n",
    "* Dropout: parametrizado en el modelo (ya incluido)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fbd2f8f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def l1_penalty(model: nn.Module) -> torch.Tensor:\n",
    "    total = torch.tensor(0., device=device)\n",
    "    for p in model.parameters():\n",
    "        if p.requires_grad:\n",
    "            total = total + p.abs().sum()\n",
    "    return total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "254ae999",
   "metadata": {},
   "source": [
    "## Task 5 — “Algoritmos de optimización”"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72ebf7a",
   "metadata": {},
   "source": [
    "* Batch GD: actualiza con todo el conjunto de entrenamiento (batch único).\n",
    "* Mini-Batch GD: actualiza por lotes pequeños (p. ej. 16).\n",
    "* SGD: actualiza por cada muestra (batch_size=1).\n",
    "\n",
    "> Usaremos el mismo optimizador (torch.optim.SGD) pero cambiaremos el tamaño del batch para reflejar cada técnica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "73ab957a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_loader(dataset, mode: str, batch_size: int = 16, shuffle: bool = True):\n",
    "    mode = mode.lower()\n",
    "    if mode == \"batch_gd\":\n",
    "        bs = len(dataset)\n",
    "    elif mode == \"sgd\":\n",
    "        bs = 1\n",
    "    elif mode == \"mini-batch\" or mode == \"mini_batch\":\n",
    "        bs = batch_size\n",
    "    else:\n",
    "        raise ValueError(\"mode debe ser 'batch_gd', 'mini-batch' o 'sgd'\")\n",
    "    return DataLoader(dataset, batch_size=bs, shuffle=shuffle)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f916a571",
   "metadata": {},
   "source": [
    "## Task 6 — Experimentación y Análisis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b7d3jtj4a9u",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ExperimentConfig:\n",
    "    loss_fn: str = \"cross_entropy\"\n",
    "    l1_reg: float = 0.0\n",
    "    l2_reg: float = 0.0\n",
    "    dropout: float = 0.0\n",
    "    opt_mode: str = \"mini_batch\"\n",
    "    lr: float = 0.001\n",
    "    epochs: int = 200\n",
    "    hidden: List[int] = None\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        if self.hidden is None:\n",
    "            self.hidden = [32, 16]\n",
    "\n",
    "def train_model(config: ExperimentConfig, train_ds, val_ds, device=device, verbose=False):\n",
    "    \"\"\"Training function optimized for Metal/MPS\"\"\"\n",
    "    model = MLP(\n",
    "        in_dim=4, \n",
    "        hidden=config.hidden, \n",
    "        out_dim=3, \n",
    "        dropout_p=config.dropout\n",
    "    ).to(device)\n",
    "    \n",
    "    loss_adapter = make_loss(config.loss_fn)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=config.lr, weight_decay=config.l2_reg)\n",
    "    \n",
    "    train_loader = make_loader(train_ds, config.opt_mode, batch_size=16)\n",
    "    val_loader = make_loader(val_ds, \"batch_gd\", shuffle=False)\n",
    "    \n",
    "    history = {\n",
    "        'train_loss': [], 'val_loss': [],\n",
    "        'train_acc': [], 'val_acc': [],\n",
    "        'train_f1': [], 'val_f1': []\n",
    "    }\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    for epoch in range(config.epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss, train_correct, train_total = 0.0, 0, 0\n",
    "        train_preds, train_targets = [], []\n",
    "        \n",
    "        for X_batch, y_batch in train_loader:\n",
    "            # Ensure tensors are on the correct device (Metal/MPS)\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            logits = model(X_batch)\n",
    "            loss = loss_adapter(logits, y_batch)\n",
    "            \n",
    "            # Add L1 regularization if specified\n",
    "            if config.l1_reg > 0:\n",
    "                loss = loss + config.l1_reg * l1_penalty(model)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item() * len(X_batch)\n",
    "            train_correct += (logits.argmax(1) == y_batch).sum().item()\n",
    "            train_total += len(X_batch)\n",
    "            \n",
    "            # Collect predictions for F1 score (move to CPU for sklearn)\n",
    "            train_preds.extend(logits.argmax(1).cpu().numpy())\n",
    "            train_targets.extend(y_batch.cpu().numpy())\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss, val_correct, val_total = 0.0, 0, 0\n",
    "        val_preds, val_targets = [], []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in val_loader:\n",
    "                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                logits = model(X_batch)\n",
    "                loss = loss_adapter(logits, y_batch)\n",
    "                \n",
    "                if config.l1_reg > 0:\n",
    "                    loss = loss + config.l1_reg * l1_penalty(model)\n",
    "                \n",
    "                val_loss += loss.item() * len(X_batch)\n",
    "                val_correct += (logits.argmax(1) == y_batch).sum().item()\n",
    "                val_total += len(X_batch)\n",
    "                \n",
    "                val_preds.extend(logits.argmax(1).cpu().numpy())\n",
    "                val_targets.extend(y_batch.cpu().numpy())\n",
    "        \n",
    "        # Calculate metrics\n",
    "        train_acc = train_correct / train_total\n",
    "        val_acc = val_correct / val_total\n",
    "        train_f1 = f1_score(train_targets, train_preds, average='weighted')\n",
    "        val_f1 = f1_score(val_targets, val_preds, average='weighted')\n",
    "        \n",
    "        history['train_loss'].append(train_loss / train_total)\n",
    "        history['val_loss'].append(val_loss / val_total)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        history['train_f1'].append(train_f1)\n",
    "        history['val_f1'].append(val_f1)\n",
    "        \n",
    "        if verbose and (epoch + 1) % 50 == 0:\n",
    "            print(f\"Epoch {epoch+1:3d} | \"\n",
    "                  f\"Train Loss: {train_loss/train_total:.4f} | \"\n",
    "                  f\"Val Loss: {val_loss/val_total:.4f} | \"\n",
    "                  f\"Train Acc: {train_acc:.4f} | \"\n",
    "                  f\"Val Acc: {val_acc:.4f}\")\n",
    "    \n",
    "    total_time = time.time() - start_time\n",
    "    \n",
    "    return {\n",
    "        'model': model,\n",
    "        'history': history,\n",
    "        'final_train_acc': train_acc,\n",
    "        'final_val_acc': val_acc,\n",
    "        'final_train_f1': train_f1,\n",
    "        'final_val_f1': val_f1,\n",
    "        'training_time': total_time,\n",
    "        'config': config\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "kw7bkvievoo",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running 11 experiments on mps\n",
      "Experiment configurations:\n",
      " 1. Loss: cross_entropy | L1: 0.0    | L2: 0.0    | Dropout: 0.0  | Opt: mini_batch\n",
      " 2. Loss: nll          | L1: 0.0    | L2: 0.0    | Dropout: 0.0  | Opt: mini_batch\n",
      " 3. Loss: mse          | L1: 0.0    | L2: 0.0    | Dropout: 0.0  | Opt: mini_batch\n",
      " 4. Loss: cross_entropy | L1: 0.001  | L2: 0.0    | Dropout: 0.0  | Opt: mini_batch\n",
      " 5. Loss: cross_entropy | L1: 0.0    | L2: 0.001  | Dropout: 0.0  | Opt: mini_batch\n",
      " 6. Loss: cross_entropy | L1: 0.0    | L2: 0.0    | Dropout: 0.3  | Opt: mini_batch\n",
      " 7. Loss: cross_entropy | L1: 0.0    | L2: 0.0    | Dropout: 0.0  | Opt: sgd\n",
      " 8. Loss: cross_entropy | L1: 0.0    | L2: 0.0    | Dropout: 0.0  | Opt: batch_gd\n",
      " 9. Loss: cross_entropy | L1: 0.0    | L2: 0.0    | Dropout: 0.0  | Opt: mini_batch\n",
      "10. Loss: nll          | L1: 0.001  | L2: 0.0    | Dropout: 0.2  | Opt: mini_batch\n",
      "11. Loss: mse          | L1: 0.0    | L2: 0.01   | Dropout: 0.1  | Opt: batch_gd\n"
     ]
    }
   ],
   "source": [
    "# Define 9+ experimental configurations\n",
    "experiments = [\n",
    "    # Baseline: different loss functions with no regularization\n",
    "    ExperimentConfig(loss_fn=\"cross_entropy\", opt_mode=\"mini_batch\"),\n",
    "    ExperimentConfig(loss_fn=\"nll\", opt_mode=\"mini_batch\"),\n",
    "    ExperimentConfig(loss_fn=\"mse\", opt_mode=\"mini_batch\"),\n",
    "    \n",
    "    # Different regularization techniques with CrossEntropy\n",
    "    ExperimentConfig(loss_fn=\"cross_entropy\", l1_reg=0.001, opt_mode=\"mini_batch\"),\n",
    "    ExperimentConfig(loss_fn=\"cross_entropy\", l2_reg=0.001, opt_mode=\"mini_batch\"),\n",
    "    ExperimentConfig(loss_fn=\"cross_entropy\", dropout=0.3, opt_mode=\"mini_batch\"),\n",
    "    \n",
    "    # Different optimization methods with CrossEntropy\n",
    "    ExperimentConfig(loss_fn=\"cross_entropy\", opt_mode=\"sgd\"),\n",
    "    ExperimentConfig(loss_fn=\"cross_entropy\", opt_mode=\"batch_gd\"),\n",
    "    ExperimentConfig(loss_fn=\"cross_entropy\", opt_mode=\"mini_batch\"),\n",
    "    \n",
    "    # Extra combinations for bonus points\n",
    "    ExperimentConfig(loss_fn=\"nll\", l1_reg=0.001, dropout=0.2, opt_mode=\"mini_batch\"),\n",
    "    ExperimentConfig(loss_fn=\"mse\", l2_reg=0.01, dropout=0.1, opt_mode=\"batch_gd\"),\n",
    "]\n",
    "\n",
    "print(f\"Running {len(experiments)} experiments on {device}\")\n",
    "print(\"Experiment configurations:\")\n",
    "for i, config in enumerate(experiments):\n",
    "    print(f\"{i+1:2d}. Loss: {config.loss_fn:<12} | \"\n",
    "          f\"L1: {config.l1_reg:<6} | L2: {config.l2_reg:<6} | \"\n",
    "          f\"Dropout: {config.dropout:<4} | Opt: {config.opt_mode}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "84whe6rani7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting experimental runs...\n",
      "================================================================================\n",
      "\n",
      "Experiment 1/11: cross_entropy + mini_batch + L1=0.0 + L2=0.0 + dropout=0.0\n",
      "✓ Completed in 5.01s | Final Val Acc: 0.6579 | Final Val F1: 0.5610\n",
      "\n",
      "Experiment 2/11: nll + mini_batch + L1=0.0 + L2=0.0 + dropout=0.0\n",
      "✓ Completed in 3.13s | Final Val Acc: 0.6316 | Final Val F1: 0.5011\n",
      "\n",
      "Experiment 3/11: mse + mini_batch + L1=0.0 + L2=0.0 + dropout=0.0\n",
      "✓ Completed in 3.71s | Final Val Acc: 0.5000 | Final Val F1: 0.4082\n",
      "\n",
      "Experiment 4/11: cross_entropy + mini_batch + L1=0.001 + L2=0.0 + dropout=0.0\n",
      "✓ Completed in 4.64s | Final Val Acc: 0.6316 | Final Val F1: 0.5883\n",
      "\n",
      "Experiment 5/11: cross_entropy + mini_batch + L1=0.0 + L2=0.001 + dropout=0.0\n",
      "✓ Completed in 3.23s | Final Val Acc: 0.6842 | Final Val F1: 0.5987\n",
      "\n",
      "Experiment 6/11: cross_entropy + mini_batch + L1=0.0 + L2=0.0 + dropout=0.3\n",
      "✓ Completed in 3.69s | Final Val Acc: 0.8158 | Final Val F1: 0.8126\n",
      "\n",
      "Experiment 7/11: cross_entropy + sgd + L1=0.0 + L2=0.0 + dropout=0.0\n",
      "✓ Completed in 38.19s | Final Val Acc: 0.9211 | Final Val F1: 0.9209\n",
      "\n",
      "Experiment 8/11: cross_entropy + batch_gd + L1=0.0 + L2=0.0 + dropout=0.0\n",
      "✓ Completed in 2.00s | Final Val Acc: 0.3158 | Final Val F1: 0.1516\n",
      "\n",
      "Experiment 9/11: cross_entropy + mini_batch + L1=0.0 + L2=0.0 + dropout=0.0\n",
      "✓ Completed in 4.28s | Final Val Acc: 0.6316 | Final Val F1: 0.5028\n",
      "\n",
      "Experiment 10/11: nll + mini_batch + L1=0.001 + L2=0.0 + dropout=0.2\n",
      "✓ Completed in 4.69s | Final Val Acc: 0.8158 | Final Val F1: 0.8087\n",
      "\n",
      "Experiment 11/11: mse + batch_gd + L1=0.0 + L2=0.01 + dropout=0.1\n",
      "✓ Completed in 1.50s | Final Val Acc: 0.2895 | Final Val F1: 0.1544\n",
      "\n",
      "================================================================================\n",
      "All experiments completed!\n",
      "\n",
      "Experimental Results Summary:\n",
      " Experiment Loss Function  L1 Reg  L2 Reg  Dropout  Optimizer Train Acc Val Acc Train F1 Val F1 Time (s)\n",
      "          1 cross_entropy   0.000   0.000      0.0 mini_batch    0.7232  0.6579   0.6512 0.5610     5.01\n",
      "          2           nll   0.000   0.000      0.0 mini_batch    0.6786  0.6316   0.5577 0.5011     3.13\n",
      "          3           mse   0.000   0.000      0.0 mini_batch    0.4821  0.5000   0.3949 0.4082     3.71\n",
      "          4 cross_entropy   0.001   0.000      0.0 mini_batch    0.7321  0.6316   0.7179 0.5883     4.64\n",
      "          5 cross_entropy   0.000   0.001      0.0 mini_batch    0.6964  0.6842   0.6152 0.5987     3.23\n",
      "          6 cross_entropy   0.000   0.000      0.3 mini_batch    0.7857  0.8158   0.7809 0.8126     3.69\n",
      "          7 cross_entropy   0.000   0.000      0.0        sgd    0.9643  0.9211   0.9643 0.9209    38.19\n",
      "          8 cross_entropy   0.000   0.000      0.0   batch_gd    0.3393  0.3158   0.1719 0.1516     2.00\n",
      "          9 cross_entropy   0.000   0.000      0.0 mini_batch    0.6696  0.6316   0.5409 0.5028     4.28\n",
      "         10           nll   0.001   0.000      0.2 mini_batch    0.8214  0.8158   0.8107 0.8087     4.69\n",
      "         11           mse   0.000   0.010      0.1   batch_gd    0.2857  0.2895   0.1790 0.1544     1.50\n"
     ]
    }
   ],
   "source": [
    "# Run all experiments (optimized for Metal/MPS)\n",
    "results = []\n",
    "print(\"Starting experimental runs...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for i, config in enumerate(experiments):\n",
    "    print(f\"\\nExperiment {i+1}/{len(experiments)}: \"\n",
    "          f\"{config.loss_fn} + {config.opt_mode} + \"\n",
    "          f\"L1={config.l1_reg} + L2={config.l2_reg} + dropout={config.dropout}\")\n",
    "    \n",
    "    # Ensure fresh random state for each experiment\n",
    "    set_seed(42 + i)\n",
    "    \n",
    "    # Run training\n",
    "    result = train_model(config, train_ds, val_ds, device=device, verbose=False)\n",
    "    results.append(result)\n",
    "    \n",
    "    print(f\"✓ Completed in {result['training_time']:.2f}s | \"\n",
    "          f\"Final Val Acc: {result['final_val_acc']:.4f} | \"\n",
    "          f\"Final Val F1: {result['final_val_f1']:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"All experiments completed!\")\n",
    "\n",
    "# Create summary table\n",
    "summary_data = []\n",
    "for i, result in enumerate(results):\n",
    "    config = result['config']\n",
    "    summary_data.append({\n",
    "        'Experiment': i + 1,\n",
    "        'Loss Function': config.loss_fn,\n",
    "        'L1 Reg': config.l1_reg,\n",
    "        'L2 Reg': config.l2_reg,\n",
    "        'Dropout': config.dropout,\n",
    "        'Optimizer': config.opt_mode,\n",
    "        'Train Acc': f\"{result['final_train_acc']:.4f}\",\n",
    "        'Val Acc': f\"{result['final_val_acc']:.4f}\",\n",
    "        'Train F1': f\"{result['final_train_f1']:.4f}\",\n",
    "        'Val F1': f\"{result['final_val_f1']:.4f}\",\n",
    "        'Time (s)': f\"{result['training_time']:.2f}\"\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "print(\"\\nExperimental Results Summary:\")\n",
    "print(summary_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e754d5",
   "metadata": {},
   "source": [
    "# Parte 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "314041e3",
   "metadata": {},
   "source": [
    "1. **¿Cuál es la principal innovación de la arquitectura Transformer?**\n",
    "\n",
    "  La gran innovación del Transformer es eliminar por completo las recurrencias y convoluciones. En su lugar, se basa únicamente en mecanismos de atención, especialmente *self-attention*, para modelar dependencias entre tokens. Esto permite mayor paralelización en el entrenamiento y mejor manejo de dependencias largas.\n",
    "\n",
    "2. **¿Cómo funciona el mecanismo de atención del scaled dot-product?**\n",
    "\n",
    "  El scaled dot-product attention toma consultas (Q), claves (K) y valores (V). Calcula los productos punto entre las consultas y todas las claves, los escala por $1/\\sqrt{d_k}$ para evitar gradientes muy pequeños, y aplica softmax para obtener pesos de atención. Luego usa esos pesos para combinar linealmente los valores:\n",
    "\n",
    "  $$\n",
    "  \\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\n",
    "  $$\n",
    "\n",
    "  Este escalado es lo que diferencia al mecanismo y lo hace más estable.\n",
    "\n",
    "3. **¿Por qué se utiliza la atención de múltiples cabezales en Transformer?**\n",
    "\n",
    "  La multi-head attention proyecta las Q, K y V en distintos subespacios, aplica atención en paralelo y concatena los resultados. Esto permite que el modelo aprenda a atender a diferentes aspectos de la información en paralelo, como relaciones sintácticas y semánticas distintas. Con un único “head”, la información se promediaría y se perderían matices\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
